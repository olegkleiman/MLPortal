const translations = {
    en: {
        translation: {
            "Switch": "Russian",
            "PortalName": "ML Portal",
            "Intro": "Introduction",
            "Regression": "Regression",
            "Classification": "Classification",
            "RigdeRegression": "Rigde Regression",
            "GradientDescent": "Gradient Descent",
            "BackProp": "Back Propagation",
            "classification": {
                "1": "Classification is the task of supervised learning with <strong>discrete</strong> target variable.",
                "2": " - исходное значение из выборки",
                "3": " - значение, полученное алгоритмом приближения",
                "4": " The error calculated such a way depends on the weights and hence there is a loss funtion that usually expressed as:",
                "41": " Finding minimum of this function is actually its deriving with respect to each weight",  
                "42": "Этот поиск минимума, в свою очередь, производится методом ", 
                "43": " gradient descent",
                "shuffle": "Shuffle",
                "weights": "Weights"
            },
            "regression": {
                "1": "Regtesssoin is the task of supervised learning where with continious target variable"
            },
            "grad": {
                "1": "The main idea behind the gradient descent method is updating the vector of weights by the value opposite to gradient direction:",
                "2": " - network parameters at ",
                "3": " iteration of gradient descent",
                "4": "This is a vector equation where each element is changed as part of vector",
                "5": ", thus, if ",
                "6": "Recap that что мы минимизируем среднеквадратичную функцию потерь", 
                "7": " - значение элемента выборки для пары",  
                "8": "Further, according to the chain rule",
                "9": " - summation (dot product of weights and bias) of the neuron",
                "10": " at layer ",
                "11": " before the value of summation is passed to activation function (in our case - sigmoid).",
                "12": "Первая часть этого выражения обычно называется поправкой (по соображениям изложенным ниже) and denoted:",
                "13": "Второй множитель последнего уравнения может быть вычислен следующим образом:",  
                "14": "- output of the element",  
                "15": " at layer",
                "16": "- number of elements at layer",
                "17": "It is very important result. Basically it says that partial derivative with respect to weight is the error ", 
                "18": "at unit",
                "19": " multiplied to output",
                "20": " of unit",
                "21": "На интуитивном уровне это понятно, because the weight ",
                "22": " connects the output of unit",
                "23": " to input of unit",
                "24": "It should be noted that полученное выражение для частной производной получены в общем виде, не подразумевая никаких допущений о функции активации. Что же касается функции ошибки, то очевидно, она должна быть определена для каждого конкретного случая",
                
            },
            "backprop": {
                "1": "Этот метод является одним из самых главных алгоритмов машинного обучения. Он является алгоритмом вычисления ",
                "2": "градиентного спуска",
            },
            "where": "where ",
            "and": " and ",
            "_and": " a ",
            "thus": "Тhus",
            "then": ", then ",
            "solve": "Solve",
        }
    },
    ru: {
        translation: {
            "Switch": "English",
            "PortalName": "ML Portal (ru)",
            "Intro": "Введение",
            "Regression": "Регрессия",
            "Classification": "Классификация",
            "RigdeRegression": "Гребневая регрессия",
            "GradientDescent": "Градиентный спуск",
            "BackProp": "Метод обратного распространения ошибки",
            "classification": {
                "1": "Классификация является задачей машинного обучения с <strong>дискретным</strong> набором выходных значений (меток). Как и для задачи регрессии, основой решения будет воссоздание функции зависимости. Для этого строится разделяющая прямая/плоскость/гипер-плоскость/гипер-поверхность, которая является приближением исходной зависимости в смысле минимизации средне-квадратичного отклонения ошибки:",
                "2": " - исходное значение из выборки",
                "3": " - значение, полученное алгоритмом приближения.",
                "4": "Поскольку ошибка, рассчитываемая таким образом, зависит от весов, говорят о функции потерь (loss funtion), как зависящей от весов и её обозначают ",
                "41": "Минимизация этой функции сводится к нахождению ее производных по каждому из весов:",
                "42": "Этот поиск минимума, в свою очередь, производится методом ",
                "43": " градиентного спуска",
                "shuffle": "Новые данные",
                "weights": "Веса"
            },
            "regression": {
                "1": "Regtesssoin is the task of supervised learning where with continious target variable"
            },
            "grad": {
                "1": "Основная идея метода градиентного спуска состоит в обновлении вектора весов на величину, обратную к направлению градиента:",
                "2": " - параметры сети на ",
                "3": " итерации градиентного спуска",
                "4": "Это векторная запись уравнения, в котором производится изменение каждого элемента вектора",
                "5": ", т.е. если ",
                "6": "Напомним далее, что мы минимизируем среднеквадратичную функцию потерь",
                "7": " - значение элемента выборки для пары",
                "8": "Далее, по правилу дифференцирования сложной функции",
                "9": " - сумматор (скалярное произведение весов и смещение) нейрона",
                "10": " в слое ",
                "11": " до того, как значение сумматора будет передано в функцию активации (в нашем случае, в сигмоид).",
                "12": "Первая часть этого выражения обычно называется поправкой (по соображениям изложенным ниже) и обозначается:", 
                "13": "Второй множитель последнего уравнения может быть вычислен следующим образом:",
                "14": "- выход элемента",
                "15": " на слое ",
                "16": "- число элементов на слое",
                "17": "Это очень важный результат. Он говорит, что частная производная ошибки по весу есть поправка ",
                "18": " на ячейке",
                "19": " умноженная на выход",
                "20": " ячейки",
                "21": "На интуитивном уровне это понятно, так как вес ",
                "22": " связывает выход ячейки",
                "23": " со входом ячейки",
                "24": "Особенно важно отметить, что полученное выражение для частной производной получены в общем виде, не подразумевая никаких допущений о функции активации. Что же касается функции ошибки, то очевидно, она должна быть определена для каждого конкретного случая",
            },
            "backprop": {
                "1": "Этот метод является одним из самых главных алгоритмов машинного обучения. Он является алгоритмом вычисления ",
                "2": "градиентного спуска",
            },
            "where": "где ",
            "and": " и ",
            "_and": " a ",
            "thus": "Таким образом",
            "then": ", то ",
            "solve": "Решить",
        }
    }
}

export default translations;
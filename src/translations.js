const translations = {
    en: {
        translation: {
            "Switch": "Russian",
            "Home": "Home",
            "PortalName": "ML Portal",
            "Intro": "Introduction",
            "Regression": "Regression",
            "Classification": "Classification",
            "RigdeRegression": "Rigde Regression",
            "GradientDescent": "Gradient Descent",
            "BackProp": "Back-Propagation",
            "Graph": "Computational Graph",
            "ANN": "Neural Networks",
            "Bayes": "Bayessian networks",
            "intro": {
                "1": "",
            },
            "classification": {
                "1": "Classification is the task of supervised learning with <strong>discrete</strong> target variable.",
                "2": " - исходное значение из выборки",
                "3": " - значение, полученное алгоритмом приближения",
                "4": " The error calculated such a way depends on the weights and hence there is a loss funtion that usually expressed as:",
                "41": " Finding minimum of this function is actually its deriving with respect to each weight",  
                "42": "Этот поиск минимума, в свою очередь, производится методом ", 
                "43": " gradient descent",
                "44": "Для задач классификации в качестве функция активации, как правило,  используется softmax, а в качестве функции ошибки - cross-entropy",
                "shuffle": "Shuffle",
                "weights": "Weights"
            },
            "reg": {
                "topic_0": "Regression Task",
                "topic_1": "Classical closed-form solution",
                "topic_2": "L2 Regularization (Ridge)",
                "topic_3": "Complexity",
                "1": "Машинное обучение смотрит на задачу регрессии как на форму обучения с учителем. В качестве меток здесь принимаются значения целевой функции. Regresssoin is the task of supervised learning where with continious target variable",
                "2": "This expression is called <strong>normal equation</strong> and it is closed-form of the solution for linear regression",
                "ridge": {
                    "0": "Closed-form solution",
                    "1": "Ridge regression",
                    "2": "If the parametrs of regression are constrained with some quadratic expression, its solutions are restricted by the area defined by such constraints. If the constraints are imposed using a quadratic function",
                    "3": " with parameter ",
                    "4": " (in this case the regression told to be regularized by ",
                    "5": "), the possible solutions are limited by the multidimensional sphere with radius depends on ",
                    "6": " Renaming loss functions and adding mentioned constraints:",
                    "7": "The regresssion described by last equation called ridge regression, its ridge is diagonal matrix that is added to matrix",
                    "8": "The matrix produced by this add opertion is surely regular",
                },
                "3": " Till now we got a closed-form solutions both types of regressions. Now we can estimate the complexity of these solutions.",
                "4": "The normal equation implies one operation to calculate the inverse matrix, 2 operations of matrix transposition and 3 matrix multiplication operations. Matrix inversion requires ",
                "5": " operations. Matrix multiplication ",
                "6": " requires ",
                "7": " operations",
                "8": " It's called 'normal' because the dimentionality of its right part does not depends on ",
                "9": " one-dimension real vector (after all, it's the vector of weights)",
                "10": " since inversion does not change dimesionality for square matix",
                "11": " because sigle element of produced matrix requires ",
                "12": " operations and the total of elements is ",
                "13": "Note that normal equation assumes invertible matrix ",  
                "14": " This restriction will be removed in the next part by imposing some constraints on regression parameters",
                "15": " Even for relatively small matrices 28х28 we need to perform 28^3 + (28*28)*2 operations. Here the gradient descent methods comes to help.",
                "16": "При всей красоте аналитического решения не забудем, что оно может быть применено только для линейных зависимостей. Именно поэтому, а не из-за его вычислительной сложности, мы не будем применять его в дальнейшем. Вместо этого, мы воспользуемся одним из численных методов для вычисления минимума рассмотренной функции потерь - методом градиентного спуска. Мы применим этот метод к специальной конструкции - нейронной сети, свойства которой позволяют ей обучаться, т.е. находить искомые веса всё с лучшим приближением. Метод дифференцирования функции потерь, который будет применен нами в градиентном спуске, называется обратным распространением ошибки. Именно с помощью этого метода, нейронные сети нашли такое широкое применение сегодня, хотя область его применения не ограничивается только нейронными сетями.",
            },
            "grad": {
                "1": "The main idea behind the gradient descent method is updating the vector of weights by the value opposite to gradient direction:",
                "2": " - network parameters at ",
                "3": " iteration of gradient descent",
                "4": "Note that this is a vector equation where each element is changed as part of vector",
                "41": " - descent step - a small number that selected empirically",
                "42": "  Usually 0.01 is good strating choice, but from the theretical point of view, the step is selected by Lipschitz constraints:",
                "421": "function ",
                "43": " The differentiable function is assumed to be convex( although ",
                "44": "in this paper ",
                "45": "the authors consider the convergence of the method for non-convex functions)",
                "5": ", thus, if ",
                "6": "Recap that что мы минимизируем среднеквадратичную loss (cost) function", 
                "7": " - the value of sample for pair", 
                "71": "Также важно отметить, что для осуществления одного шага по классическому градиентному спуску, необходимо рассчитать значения всех производных, т.е. вычислить Якобиан функции в точке ",                 
                "8": "Further, according to the chain rule",
                "9": " - summation (dot product of weights and bias) of the neuron",
                "10": " at layer ",
                "11": " before the value of summation is passed to activation function (in our case - sigmoid).",
                "12": "First term is usually called the <strong>error</strong> (по соображениям изложенным ниже) and denoted:",
                "13": "Second term последнего уравнения может быть вычислен следующим образом:",  
                "14": "- output of the element",  
                "15": " at layer",
                "16": "- number of elements at layer",
                "17": "It is very important result. Basically it says that partial derivative with respect to weight is the error ", 
                "171": " - the value calculated by network for input ",
                "18": "at unit",
                "19": " multiplied to output",
                "20": " of unit",
                "21": "На интуитивном уровне это понятно, because the weight ",
                "22": " connects the output of unit",
                "23": " to input of unit",
                "24": "For a meanwhile полученное выражение для частной производной получены в общем виде, не подразумевая никаких допущений о функции активации. Что же касается функции ошибки, то очевидно, она должна быть определена для каждого конкретного случая",
                "25": " eigenvestor estimator for operator",
                "26": " guarantees decrease",
            },
            "backprop": {
                "1": "Этот метод является одним из самых главных алгоритмов машинного обучения. Он является алгоритмом вычисления ",
                "2": "градиентного спуска",
                "3": " The term 'back-propagation' is often misunderstood as meaning the whole learning algorithm for the network. Actually, back-propagation refers only to the method for computing the gradient, while another algorithms, such as SGD, is used to perform learning using this gradient.",
                "4": "Applying ", 
                "5": "gradient descent method ",
                "6": " for miniization loss function defined as mean square error",
                "7": "Now according to the chain rule of calculus:",
            },
            "graph": {
                "1": "Computational graph is used for calculating the coefficients in ",
                "2": "backpropagation method",
                "3": "Each node in the graph is used to indicate a variable - scalars, vectors, matrices or tensors. Edges are the operations of them.",
                "4": "",
            },
            "ann": {
                "1": "The main component of Neural Network is perceptron",
            },
            "bayes": {
                "1": "Bayesian theorem is derived simply from the conditional probalility definition",
                "2": "Откуда следует, что если",
                "3": "In this expression ",
                "4": " defines the a-posteriori probablility of the hypothesis ",
                "5": " - the apriori probalility ",
                "6": " - the likelihood of the hypothesis ",
                "7": " with defined",
                "8": " In other words, Bayes theorem simply simply expresses the fact that aposteriori probability is proportional to likelihood",
                "9": "It follows directly from here that by maximizing the likelihood we are actually maximizing the posterior probability",
                "10": " вычисляется по формуле полной вероятности события, зависящего от нескольких несовместимых гипотез, имеющих суммарную вероятность 1",
                "sample": {
                    "1": "There are consecutive numbers in three boxes. In the first box, there are numbers 1,2,3,4. In the seconds box - 1,2,3,4,5,6. In the third box - 1,2,3,4,5,6,7,8"
                }
            },
            "if": "if",
            "where": "where ",
            "and": " and ",
            "_and": " a ",
            "thus": "Тhus",
            "so_that": "so that",
            "then": ", then ",
            "although": "although",
            "solve": "Solve",
            "select": "Select",
            "activation_function": "Activation Function",
            "indeed": "Indeed",
            "sample": "Sample"
        }
    },
    ru: {
        translation: {
            "Switch": "English",
            "Home": "",
            "PortalName": "ML Portal (ru)",
            "Intro": "Введение",
            "Regression": "Регрессия",
            "Classification": "Классификация",
            "RigdeRegression": "Гребневая регрессия",
            "GradientDescent": "Градиентный спуск",
            "BackProp": "Метод обратного распространения ошибки",
            "Graph": "Вычислительный граф",
            "ANN": "Нейронные сети",
            "Bayes": "Байессовские сети",
            "intro": {
                "1": "",
            },
            "classification": {
                "1": "Классификация является задачей машинного обучения с <strong>дискретным</strong> набором выходных значений (меток). Как и для задачи регрессии, основой решения будет воссоздание функции зависимости. Для этого строится разделяющая прямая/плоскость/гипер-плоскость/гипер-поверхность, которая является приближением исходной зависимости в смысле минимизации средне-квадратичного отклонения ошибки:",
                "2": " - исходное значение из выборки",
                "3": " - значение, полученное алгоритмом приближения.",
                "4": "Поскольку ошибка, рассчитываемая таким образом, зависит от весов, говорят о функции потерь (loss funtion), как зависящей от весов и её обозначают ",
                "41": "Минимизация этой функции сводится к нахождению ее производных по каждому из весов:",
                "42": "Этот поиск минимума, в свою очередь, производится методом ",
                "43": " градиентного спуска",
                "44": "Для задач классификации в качестве функция активации, как правило,  используется softmax, а в качестве функции ошибки - cross-entropy",                "shuffle": "Новые данные",
                "weights": "Веса"
            },
            "reg": {
                "topic_0": "Задача регрессии",
                "topic_1": "Аналитическое решение",
                "topic_2": "Ограничения на параметры (Гребневая регрессия)",
                "topic_3": "Вычислительная сложность",
                "1": "Машинное обучение смотрит на задачу регрессии как на форму обучения с учителем. В качестве меток здесь принимаются значения целевой функции. Regresssoin is the task of supervised learning where with continious target variable",
                "2": "Полученное выражение называется <strong>normal equation</strong> представляет собой аналитическую форму решения задачи линейной регрессии",
                "ridge": {
                    "0": "Аналитическое решение",
                    "1": "Гребневая регрессия",
                    "2": "Если добавить ограничения на параметры регрессии, то решение такой задачи будет лежать только в области, определенной этими ограничениями. В частности, если ограничения наложены с помощью квадратичной функции ",
                    "3": " с параметром ",
                    "4": " (в этом случае говорят об ограничениях вида ",
                    "5": "), то решения будут ограничены некоторой окружностью, радиус которой зависит от ",
                    "6": "Переименуем функцию потерь и добавим в неё ограничения:",
                    "7": "Регрессия, описываемая последней формулой, называется гребневой. А гребнем выступает диагональная матрица, которую мы прибавляем к матрице",
                    "8": "в результате получается гарантированно регулярная матрица",
                },
                "3": "Теперь, когда мы получили аналитические решения для обоих случаев регрессии, мы можем оценить их вычислительную сложность.",
                "4": "Из формулы нормального уравнения следует, что нам потребуется одна операция вычисления обратной матрицы, 2 операции транспонирования и 3 операции умножения. Вычисление обратной матрицы требует  ",
                "5": " операций. Умножение матриц размера ",
                "6": " требует ",
                "7": " операций",
                "8": " Оно называется 'нормальным' потому, что размерность его правой части на зависит от ",
                "9": "вещественный вектор (это вектор весов)",
                "10": " поскольку операция получения обратной матрицы не изменяет размерность квадратной матрицы",
                "11": " т.к. для получения одного элемента выходной матрицы требуется ",
                "12": " операций, а всего элементов матрицы - ",
                "13": "Отметим, что полученное уравнение предполагает невырожденность матрицы ",
                "14": " Это ограничение мы снимем в следующей части, введя дополнительные требования к параметрам регрессии",
                "15": " Даже для относительно небольших матриц 28х28 нам потребуется произвести 28^3 + (28*28)*2 операций",
                "16": "При всей красоте аналитического решения не забудем, что оно может быть применено только для линейных зависимостей. Именно поэтому, а не из-за его вычислительной сложности, мы не будем применять его в дальнейшем. Вместо этого, мы воспользуемся одним из численных методов для вычисления минимума рассмотренной функции потерь - методом градиентного спуска. Мы применим этот метод к специальной конструкции - нейронной сети, свойства которой позволяют ей обучаться, т.е. находить искомые веса всё с лучшим приближением. Метод дифференцирования функции потерь, который будет применен нами в градиентном спуске, называется обратным распространением ошибки. Именно с помощью этого метода, нейронные сети нашли такое широкое применение сегодня, хотя область его применения не ограничивается только нейронными сетями.",            },
            "grad": {
                "1": "Основная идея метода градиентного спуска состоит в обновлении вектора весов на величину, обратную к направлению градиента:",
                "2": " - параметры сети на ",
                "3": " итерации градиентного спуска",
                "4": "Обратим внимание на то, что [1] - это векторная запись уравнения, в котором производится изменение каждого элемента вектора",
                "41": " - шаг спуска - число, которое выбирают эмпирически",
                "42": " Обычно можно начать с 0.01, но с теоретической точки зрения шаг спуска подбирают на основании условий Липшица:",
                "421": "функция ",
                "43": " Дифференцируемая функция предполагается выпуклой (хотя ",
                "44": "в этой работе ",
                "45": "авторы рассматривают сходимость метода и для невыпуклых функций)", 
                "5": ", т.е. если ",
                "6": "Напомним далее, что мы минимизируем среднеквадратичную функцию потерь и",
                "7": " - значение элемента выборки для пары",
                "71": "Также важно отметить, что для осуществления одного шага по классическому градиентному спуску, необходимо рассчитать значения всех частных производных, т.е. вычислить Якобиан функции в точке ",
                "8": "Далее, по правилу дифференцирования сложной функции",
                "9": " - сумматор (скалярное произведение весов и смещение) нейрона",
                "10": " в слое ",
                "11": " до того, как значение сумматора будет передано в функцию активации (в нашем случае, в сигмоид).",
                "12": "Первая часть этого выражения обычно называется поправкой (по соображениям изложенным ниже) и обозначается:", 
                "13": "Второй множитель последнего уравнения может быть вычислен следующим образом:",
                "14": "- выход элемента",
                "15": " на слое ",
                "16": "- число элементов на слое",
                "17": "Это очень важный результат. Он говорит, что частная производная ошибки по весу есть поправка ",
                "171": " - вычисленное значение сети для входа ",
                "18": " на ячейке",
                "19": " умноженная на выход",
                "20": " ячейки",
                "21": "На интуитивном уровне это понятно, так как вес ",
                "22": " связывает выход ячейки",
                "23": " со входом ячейки",
                "24": "Пока что полученное выражение для частной производной получены в общем виде, не подразумевая никаких допущений о функции активации. Что же касается функции ошибки, то очевидно, она должна быть определена для каждого конкретного случая",
                "25": " оценка собственных значений оператора",
                "26": " гарантирует убывание",
            },
            "backprop": {
                "1": "Этот метод является одним из самых главных алгоритмов машинного обучения. Он является алгоритмом вычисления ",
                "2": "градиентного спуска",
                "3": "Обратим внимание, что часто под термином 'back-propagation' неправильно понимается весь алгоритм обучения сети, в том время как back-propagation в общем виде вычисляет градиент функции, а для обучения применяется другой алгоритм, (такой как, например, SGD), который использует вычисленный градиент.",
                "4": "Применим ",
                "5": "метод градиентного спуска ",
                "6": "для минимизации среднеквадратичой функции потерь",
                "7": "Далее, по правилу дифференцирования сложной функции",
            },
            "graph": {
                "1": "Вычислительный граф используется для расчета коэффициентов в методе ",
                "2": "обратного распространения ошибки",
                "3": "Узлами графа являются переменные - скаляры, векторы, матрицы и тензоры. Ребрами - операции над ними.",
                "4": "Если у нас получится представить сложную функцию как композицию более простых, то мы сможем и эффективно вычислить ее производную по любой переменной, что и требуется для градиентного спуска. Самое удобное представление в виде композиции — это представление в виде графа вычислений. Граф вычислений — это граф, узлами которого являются функции (обычно достаточно простые, взятые из заранее фиксированного набора), а ребра связывают функции со своими аргументами.",
            },
            "ann": {
                "1": "Основной компонент нейронной сети – перцептрон",
            },
            "bayes": {
                "1": "Сама по себе формула Байеса выводится из определения условной вероятности",
                "2": "Откуда следует, что если",
                "3": "В этом выражении ",
                "4": " означает апостериорную вероятность гипотезы",
                "5": " - априорную вероятность ",
                "6": " - правдоподобие гипотезы ",
                "7": " при заданном",
                "8": "Иными словами, формула Байеса просто утверждает, что апостериорная вероятность пропорциональна правдоподобию",
                "9": "Отсюда непосредственно следует, что максимизируя правдоподобие, мы тем самым, максимизируем апостериорную вероятность",
                "10": " вычисляется по формуле полной вероятности события, зависящего от нескольких несовместимых гипотез, имеющих суммарную вероятность 1",
                "sample": {
                    "1": "Из трех коробочек можно вытащить любую из последовательных цифр. Из первой коробочки - 1,2,3,4. Из второй - 1,2,3,4,5,6 и из третьей - 1,2,3,4,5,6,7,8. Сначала случайным образом выбирается коробочка, а затем зрителю демонстрируется выбранная цифра - 2. Найти вероятность того, что предъявленная двойка была выбрана из первой коробочки."
                }
            },
            "if": "если",
            "where": "где ",
            "and": " и ",
            "_and": " a ",
            "thus": "таким образом",
            "so_that": "что",
            "then": ", то ",
            "although": "хотя",
            "solve": "Решить",
            "select": "Выберите",
            "activation_function": "Функция активации",
            "indeed": "В самом деле",
            "sample": "Пример"
        }
    }
}

export default translations;
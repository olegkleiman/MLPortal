const translations = {
    en: {
        translation: {
            "Switch": "Russian",
            "PortalName": "ML Portal",
            "Intro": "Introduction",
            "Regression": "Regression",
            "Classification": "Classification",
            "RigdeRegression": "Rigde Regression",
            "GradientDescent": "Gradient Descent",
            "classification": {
                "1": "Classification is the task of supervised learning with <strong>discrete</strong> target variable.",
                "2": " - исходное значение из выборки",
                "3": " - значение, полученное алгоритмом приближения",
                "4": " The error calculated such a way depends on the weights and hence there is a loss funtion that usually expressed as:",
                "41": " Finding minimum of this function is actually its deriving with respect to each weight",  
                "42": "Этот поиск минимума, в свою очередь, производится методом ", 
                "43": " gradient descent",
                "shuffle": "Shuffle",
                "weights": "Weights"
            },
            "regression": {
                "1": "Regtesssoin is the task of supervised learning where with continious target variable"
            },
            "grad": {
                "1": "Основная идея метода градиентного спуска состоит в обновлении вектора весов на величину, обратную к направлению градиента:",
                "2": " - параметры сети на ",
                "3": " итерации градиентного спуска ",
                "4": "Это векторная запись уравнения, в котором производится изменение каждого элемента вектора",
                "5": ", т.е. если ",
                "6": "Напомним далее, что мы минимизируем среднеквадратичную функцию потерь", 
                "7": " - значение элемента выборки для пары",  
                "8": "Далее, по правилу дифференцирования сложной функции",
                "9": " - сумматор (скалярное произведение весов и смещение) нейрона",
                "10": " в слое ",
                "11": " до того, как значение сумматора будет передано в функцию активации (в нашем случае, в сигмоид).",
            },
            "where": "где ",
            "and": " and ",
            "thus": "Тhus",
            "then": ", then ",
            "solve": "Solve",
        }
    },
    ru: {
        translation: {
            "Switch": "English",
            "PortalName": "ML Portal (ru)",
            "Intro": "Введение",
            "Regression": "Регрессия",
            "Classification": "Классификация",
            "RigdeRegression": "Гребневая регрессия",
            "GradientDescent": "Градиентный спуск",
            "classification": {
                "1": "Классификация является задачей машинного обучения с <strong>дискретным</strong> набором выходных значений (меток). Как и для задачи регрессии, основой решения будет воссоздание функции зависимости. Для этого строится разделяющая прямая/плоскость/гипер-плоскость/гипер-поверхность, которая является приближением исходной зависимости в смысле минимизации средне-квадратичного отклонения ошибки:",
                "2": " - исходное значение из выборки",
                "3": " - значение, полученное алгоритмом приближения.",
                "4": "Поскольку ошибка, рассчитываемая таким образом, зависит от весов, говорят о функции потерь (loss funtion), как зависящей от весов и её обозначают ",
                "41": "Минимизация этой функции сводится к нахождению ее производных по каждому из весов:",
                "42": "Этот поиск минимума, в свою очередь, производится методом ",
                "43": " градиентного спуска",
                "shuffle": "Новые данные",
                "weights": "Веса"
            },
            "regression": {
                "1": "Regtesssoin is the task of supervised learning where with continious target variable"
            },
            "grad": {
                "1": "Основная идея метода градиентного спуска состоит в обновлении вектора весов на величину, обратную к направлению градиента:",
                "2": " - параметры сети на ",
                "3": " итерации градиентного спуска ",
                "4": "Это векторная запись уравнения, в котором производится изменение каждого элемента вектора",
                "5": ", т.е. если ",
                "6": "Напомним далее, что мы минимизируем среднеквадратичную функцию потерь",
                "7": " - значение элемента выборки для пары",
                "8": "Далее, по правилу дифференцирования сложной функции",
                "9": " - сумматор (скалярное произведение весов и смещение) нейрона",
                "10": " в слое ",
                "11": " до того, как значение сумматора будет передано в функцию активации (в нашем случае, в сигмоид).",

            },
            "where": "где ",
            "and": " и ",
            "thus": "Таким образом",
            "then": ", то ",
            "solve": "Решить",
        }
    }
}

export default translations;